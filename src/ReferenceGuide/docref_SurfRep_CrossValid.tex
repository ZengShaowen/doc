% Copyright 2005-2016 Airbus-EDF-IMACS-Phimeca
% Permission is granted to copy, distribute and/or modify this document
% under the terms of the GNU Free Documentation License, Version 1.2
% or any later version published by the Free Software Foundation;
% with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
% Texts.  A copy of the license is included in the section entitled "GNU
% Free Documentation License".
\renewcommand{\etapemethodo}{Resp. Surf.}
\renewcommand{\nomfichier}{docref_SurfRep_CrossValid}
\renewcommand{\titrefiche}{Assessment of the polynomial approximations by cross validation}


\Header

\MathematicalDescription{

  \underline{\textbf{Goal}} \vspace{2mm}
  
    Once a polynomial response surface $\widehat{h}(\underline{x})$ of the original numerical model $h(\underline{x})$ has been built up, it is crucial to estimate the approximation error, i.e. the discrepancy between the response surface and the true model response in terms of a suitable norm such as the $L_2$-norm:
    \begin{equation}
      Err \, \, = \, \, \left\| h(\underline{x}) \; - \; \widehat{h}(\underline{x}) \right\|_{L_2}^2\, \, = \, \, \int_{\cD} \; \left( h(\underline{x}) \; - \; \widehat{h}(\underline{x}) \right)^2  \; d\underline{x}
    \end{equation}
    where $\cD$ denotes the support of the input parameters $\underline{x}$. It is worth emphasizing that one tends to overestimate the performance of a response surface by training and evaluating it on the same data set. For instance, the model might fail to predict on new data whereas the validation on the training data yields satisfactory performance. To avoid such issue, it is important that the model error assessment is conducted on a data set which is independent of the training sample. However, any new model evaluation may be time- and memory-consuming. Therefore, error estimates which are only based on already performed computer experiments are of interest. In this context, the so-called \emph{cross validation} techniques are utilized to obtain reliable performance assessment of the response surface without additional model evaluations.\\
  
    \underline{\textbf{Principle}} \vspace{2mm}\
    
    Any cross-validation scheme consists in dividing the data sample (i.e. the experimental design) into two sub-samples that are independently and identically distributed. A metamodel $\widehat{h}(\underline{x})$ is built from one sub-sample, i.e. the \emph{training set}, and its performance is assessed by comparing its predictions to the other subset, i.e. the \emph{test set}. A single split will lead to a validation estimate. When several splits are conducted, the cross-validation error estimate is obtained by averaging over the splits.
    
    \textbf{K-fold cross-validation error estimate} \vspace{2mm}
    
    The $K$-fold cross-validation technique relies on splitting the data set $\cX$ into $K$ mutually exclusive sub-samples $\cX_1, \dots, \cX_K$ of approximately equal size. A sub-sample $\cX_i$, $i \in \{ 1, \dots, K\} $ is set aside, then the response surface $\widehat{h}^{(-\cX_i)}$ is built on the remaining sub-samples $\cX \setminus \cX_i$. The approximation error is estimated on the set-aside sample $\cX_i$:
    \begin{equation}
      Err^{(i)}  = \dfrac{1}{ |\cX_i|}  \sum\limits_{\underline{x}^{(j)} \in \cX_i} \left[ h(\underline{x}^{(j)}) - \widehat{h}^{(-\cX_i)} {(\underline{x}^{(j)})} \right]^2
    \end{equation}
    in which $h(\underline{x}^{(j)}) - \widehat{h}^{(-\cX_i)} {(\underline{x}^{(j)})}$ is the \emph{predicted residual} defined as the difference between the evaluation of $h(\cdot)$ and the prediction with $\widehat{h}^{(-\cX_i)}(\cdot)$ at $\underline{x}^{(j)}$ in the sub-sample $\cX_i$ whose cardinality is $|\cX_i|$.\\
    The approximation errors $Err^{(i)}$ are estimated with each of the sub-samples $\cX_i$, $i \in \{ 1, \dots, K\} $ being used as the validation set whereas the remaining sub-samples $\cX \setminus \cX_i$ being used for training. Finally the $K$-fold cross-validation error estimate is obtained as the average:
    \begin{equation}
	    Err_{Kfold} = \dfrac{1}{K} \sum\limits_{i=1}^{K} Err^{(i)}
    \end{equation}
    As described above, the $K$-fold error estimate can be obtained with a single split of the data $\cX$ into $K$ folds. It is worth noting that one can repeat the cross-validation multiple times using different divisions into folds to obtain better Monte Carlo estimate. This comes obviously with an additional computational cost.
  
  
  \textbf{Classical leave-one-out error estimate} \vspace{2mm}

  The \emph{leave-one-out} (LOO) cross-validation is a special case of $K$-fold cross-validation where the number of folds $K$ is chosen equal to the cardinality $N$ of the experimental design $\cX$. It is recalled that a $P$-term polynomial approximation of the model response reads:
  \begin{equation}
    \widehat{h}(\underline{x}) \, \, = \, \,  \sum_{j=0}^{P-1} \; \widehat{a}_{j} \; \psi_{j}(\underline{x})
  \end{equation}
  where the $\widehat{a}_{j}$'s are estimates of the coefficients obtained by a specific method, e.g. least squares. \\
  Let us denote by $\widehat{h}^{(-i)}$ the approximation that has been built from the experimental design $\cX \setminus \{\underline{x}^{(i)}\}$ with the $i$-th observation $\underline{x}^{(i)}$ being set aside. The predicted residual is defined as the difference between the model evaluation at $\underline{x}^{(i)}$ and its prediction based on $\widehat{h}^{(-i)}$:
  \begin{equation} \label{eq:4.3:5}
    \Delta^{(i)} \, \, = \, \,  h(\underline{x}^{(i)}) - \widehat{h}^{(-i)}(\underline{x}^{(i)})
  \end{equation}
  By repeating this process for all observations in the experimental design, one obtains the predicted residuals $\Delta^{(i)}, i = 1, \dots , N$. Finally, the LOO error is estimated as follows:
  \begin{equation}\label{eq:4.3:6}
    Err_{LOO} \, \, = \, \, \frac{1}{N} \sum_{i=1}^{N} {\Delta^{(i)}}^{2}
  \end{equation}
  Due to the linear-in-parameters form of the polynomial chaos expansion, the quantity $Err_{LOO}$ may be computed without performing further regression calculations when the PC coefficients have been estimated using the entire experimental design $\cX$. Indeed, the predicted residuals can be obtained analytically as follows:
  \begin{equation} \label{eq:4.3:7}
    \Delta^{(i)} \, = \,
    \frac{h(\underline{x}^{(i)}) - \widehat{h}(\underline{x}^{(i)})}{1 - h_i}
  \end{equation}
  where $h_i$ is the $i$-th diagonal term of the matrix $\underline{\underline{\Psi}} (\underline{\underline{\Psi}}^{\textsf{T}}\underline{\underline{\Psi}})^{-1} \underline{\underline{\Psi}}^{\textsf{T}}$ with $\underline{\underline{\Psi}}$ being the information matrix:
  \begin{equation} \label{eq:4.3:7bis}
    \underline{\underline{\Psi}} \, \, = \, \, \left\{ \psi_{j}(\underline{x}^{(i)}) \; , \; i=1,\dots,N \; , \; j = 0,\dots,P-1 \right\}
  \end{equation}
  In practice, one often computes the following normalized LOO error:
  \begin{equation} \label{eq:4.3:8bis}
    \varepsilon_{LOO} \, \, \equiv \, \, \frac{Err_{LOO}}{\hat{\Cov{\cY}}}
  \end{equation}
  where $\hat{\Cov{\mathcal{Y}}}$ denotes the empirical covariance of the response sample $\cY$:
  \begin{equation} \label{eq:4.3:4bis}
    \hat{\Cov{\mathcal{Y}}} \, \, \equiv \, \, \frac{1}{N-1} \; \sum_{i=1}^{N} \; \left( y^{(i)} \; - \;   \bar{\mathcal{Y}}  \right)^{2} \quad  \quad , \quad \quad   \bar{\mathcal{Y}} \, \, \equiv \, \, \frac{1}{N} \; \sum_{i=1}^{N} \; y^{(i)}
  \end{equation}

  \textbf{Corrected leave-one-out error estimate} \vspace{2mm}

  A penalized variant of $\varepsilon_{LOO}$ may be used in order to increase its robustness with respect to overfitting, i.e. to penalize a large number of terms in the PC expansion compared to the size of the experimental design:
  \begin{equation}
    \varepsilon_{LOO}^{*} \, \, \equiv \, \, \varepsilon_{LOO} \, T(P,N)
  \end{equation}
  The penality factor is defined by:
  \begin{equation}
    T(P,N) \, \, = \, \,   \frac{N}{N-P}  \; \left(1 \; + \; \frac{\mbox{tr} \left( \underline{\underline{C}}_{emp}^{-1}  \right) }{N} \right)
  \end{equation}
  where:
  \begin{equation} \label{eq:4.3:10bis}
    \underline{\underline{C}}_{emp} \, \, \equiv \, \, \frac{1}{N}\underline{\underline{\Psi}}^{\textsf{T}}\; \underline{\underline{\Psi}} \quad \quad , \quad \quad
    \underline{\underline{\Psi}} \, \, = \, \, \left\{ \psi_{j}(\underline{x}^{(i)}) \, \, , \, \, i=1,\dots,N \, \, , \, \, j=0,\dots,P-1 \right\}
  \end{equation}
}
{}

\Methodology{
  Within the global methodology, the methods K-fold and corrected leave-one-out cross-validation are used to assess the accuracy of a polynomial response surface of a model output prior to tackling the step C: ``Uncertainty propagation''.
}
            {
              The use of K-fold and leave-one-out cross-validation goes back to:
              \begin{itemize}
              \item D. Allen, 1971, ``The prediction sum of squares as a criterion for selecting prediction variables'', Technical Report 23, Dept. of Statistics, University of Kentucky.
              \end{itemize}
              \begin{itemize}
              \item M. Stone, 1974, ``Cross-validatory choice and assessment of statistical predictions'', Journal of the Royal Statistical Society. Series B.
  Methodological, 111--147.
              \end{itemize}
              \begin{itemize}
              \item S. Geisser, 1975, ``The predictive sample reuse method with applications'', Journal of the American Statistical Association 70 (350), 320--328.
              \end{itemize}
              \begin{itemize}
              \item G. Saporta, 2006, ``Probabilit\'{e}s, analyse des donn\'{e}es et statistique'', Editions Technip.
              \end{itemize}
              \begin{itemize}
              \item S. Arlot and A. Celisse, 2010, ``A survey of cross-validation procedures for model selection'', Statistics surveys 4, 40--79.
              \end{itemize}
              The definition of the penalized variant has been inspired from:
              \begin{itemize}
              \item O. Chapelle, V. Vapnik, and Y. Bengio, 2002, ``Model selection for small sample regression'', Machine Learning 48 (1), 9--23.
              \end{itemize}
              In the context of polynomial chaos response surface, the use of corrected LOO error estimate was investigated thoroughly in:
              \begin{itemize}
              \item G. Blatman, 2009, ``Adaptive sparse polynomial chaos expansions for uncertainty propagation and sensitivity analysis'', PhD dissertation, Universit\'e Blaise Pascal, Clermont-Ferrand, France.
              \end{itemize}
            }
